reduced$relative_lateness = reduced$relative_lateness + 70 #Adding a constant to the data before taking the log transform in order to remove the negative values
set.seed(100)
sample_data <- reduced[sample(1:nrow(reduced) , 100),]
#========================================================================================
#Cluster_performance function to calculate SSW and SSB
#========================================================================================
cluster_performance<-function(dat,
cl){
means<-sapply(1:max(dat[,cl]), function(x){
unlist(apply(dat[,1:(ncol(dat)-1)], 2, function(y){
mean(y[which(dat[,cl]==x)])
}))
})
trans_dat<-t(sapply(1:nrow(dat), function(x){
unlist(apply(dat[x,1:(ncol(dat)-1)],1,function(y){
(as.vector(y)-as.vector(means[,dat[x,cl]]))^2
}))
}))
SSW<-t(sapply(1:max(dat[,cl]), function(x){
unlist(apply(trans_dat, 2, function(y){
sum(y[which(dat[,cl]==x)])
}))
}))
colnames(SSW)<-colnames(dat[,1:(ncol(dat)-1)])
rownames(SSW)<-1:max(dat[,cl])
SSW_p_c<-apply(SSW,1,sum)
SSW_p_p<-apply(SSW,2,sum)
SSB<-sapply(1:nrow(means), function(x){
sapply(1:ncol(means), function(y){
nrow(dat[which(dat[,cl]==y),])*(means[x,y]-mean(dat[,x]))^2
})
})
colnames(SSB)<-colnames(dat[,1:(ncol(dat)-1)])
rownames(SSB)<-1:max(dat[,cl])
SSB_p_c<-apply(SSB,1,sum)
SSB_p_p<-apply(SSB,2,sum)
out<-as.matrix(cbind(sum(SSW),sum(SSB)))
colnames(out)<-c("SSW","SSB")
return(out)
}
#========================================================================================
#Standard Deviation Function to calculate SD of results
#========================================================================================
library(cluster)
library(LICORS)
library(kknn)
#Defining functions of our algorithms
#----------------------------------------------------------------------------------------
f1 <- function(x){kmeans(x, centers = 30, iter.max = 20)$cluster} #kmeans
f2 <- function(x){pam(x, k=30)$clustering} #kmedoids
f3 <- function(x){kmeanspp(x, k=30, start = "random", iter.max = 10)$cluster} #kmpp
f4 <- function(x){find_k <- train.kknn(x[,ncol(x)] ~ .,
data = x,
kmax = 35,
kernel = "gaussian",
kcv = 5)
return(specClust(x, nn = find_k$best.parameters$k, method = "symmetric")$cluster)}
f5 <- function(x){find_k <- train.kknn(x[,ncol(x)] ~ .,
data = x,
kmax = 35,
kernel = "gaussian",
kcv = 5)
return(specClust(x, nn = find_k$best.parameters$k, method = "random-walk")$cluster)}
#----------------------------------------------------------------------------------------
algos <- c(f1, f2, f3, f4, f5)
standard_deviations <- function(data, algorithm){
#data is our sample data
#algorithm is either 1, 2, 3, 4, or 5, where:
#1: kmeans 2:kmedoids 3:kmpp 4:spcc-symm 5:spcc-randomwalk
SSW <- integer(100)
SSB <- integer(100)
Time <- integer(100)
data$cluster = algos[[algorithm]](data)
cluster_performance(data, "cluster")
measures <- data.frame(SSW = SSW, SSB = SSB)
for(i in 1:10) {
t = proc.time()
data$cluster = algos[[algorithm]](data)
measures[i,] = cluster_performance(data, "cluster")
Time[i] = (proc.time() - t)[2]
}
return(cbind(measures, Time))
}
#data = f1(sample_data)
#Running Everything
#----------------------------------------------------------------------------------------
p1 = proc.time()
km_repeated = standard_deviations(sample_data, 1)
p1 = (proc.time() - p1)
km_repeated
standard_deviations <- function(data, algorithm){
#data is our sample data
#algorithm is either 1, 2, 3, 4, or 5, where:
#1: kmeans 2:kmedoids 3:kmpp 4:spcc-symm 5:spcc-randomwalk
SSW <- integer(100)
SSB <- integer(100)
Time <- integer(100)
data$cluster = algos[[algorithm]](data)
cluster_performance(data, "cluster")
measures <- data.frame(SSW = SSW, SSB = SSB)
for(i in 1:100) {
t = proc.time()
data$cluster = algos[[algorithm]](data)
measures[i,] = cluster_performance(data, "cluster")
Time[i] = (proc.time() - t)[2]
}
return(cbind(measures, Time))
}
#data = f1(sample_data)
#Running Everything
#----------------------------------------------------------------------------------------
p1 = proc.time()
km_repeated = standard_deviations(sample_data, 1)
p1 = (proc.time() - p1)
km_repeated
p2 = proc.time()
kmed_repeated = standard_deviations(sample_data, 2)
p2 = (proc.time() - p2)
kmed_repeated
p3 = proc.time()
kmpp_repeated = standard_deviations(sample_data, 3)
p3 = (proc.time() - p3)
reduced <- read.csv("C:/Users/Nura/Desktop/Jacobs/reduced.csv", stringsAsFactors=FALSE)
reduced <- reduced[,-1]
#print(reduced)
reduced$relative_lateness = reduced$relative_lateness + 70 #Adding a constant to the data before taking the log transform in order to remove the negative values
set.seed(100)
sample_data <- reduced[sample(1:nrow(reduced) , 100),]
#========================================================================================
#Cluster_performance function to calculate SSW and SSB
#========================================================================================
cluster_performance<-function(dat,
cl){
means<-sapply(1:max(dat[,cl]), function(x){
unlist(apply(dat[,1:(ncol(dat)-1)], 2, function(y){
mean(y[which(dat[,cl]==x)])
}))
})
trans_dat<-t(sapply(1:nrow(dat), function(x){
unlist(apply(dat[x,1:(ncol(dat)-1)],1,function(y){
(as.vector(y)-as.vector(means[,dat[x,cl]]))^2
}))
}))
SSW<-t(sapply(1:max(dat[,cl]), function(x){
unlist(apply(trans_dat, 2, function(y){
sum(y[which(dat[,cl]==x)])
}))
}))
colnames(SSW)<-colnames(dat[,1:(ncol(dat)-1)])
rownames(SSW)<-1:max(dat[,cl])
SSW_p_c<-apply(SSW,1,sum)
SSW_p_p<-apply(SSW,2,sum)
SSB<-sapply(1:nrow(means), function(x){
sapply(1:ncol(means), function(y){
nrow(dat[which(dat[,cl]==y),])*(means[x,y]-mean(dat[,x]))^2
})
})
colnames(SSB)<-colnames(dat[,1:(ncol(dat)-1)])
rownames(SSB)<-1:max(dat[,cl])
SSB_p_c<-apply(SSB,1,sum)
SSB_p_p<-apply(SSB,2,sum)
out<-as.matrix(cbind(sum(SSW),sum(SSB)))
colnames(out)<-c("SSW","SSB")
return(out)
}
#========================================================================================
#Standard Deviation Function to calculate SD of results
#========================================================================================
library(cluster)
library(LICORS)
library(kknn)
#Defining functions of our algorithms
#----------------------------------------------------------------------------------------
f1 <- function(x){kmeans(x, centers = 30, iter.max = 20)$cluster} #kmeans
f2 <- function(x){pam(x, k=30)$clustering} #kmedoids
f3 <- function(x){kmeanspp(x, k=30, start = "random", iter.max = 10)$cluster} #kmpp
f4 <- function(x){find_k <- train.kknn(x[,ncol(x)] ~ .,
data = x,
kmax = 35,
kernel = "gaussian",
kcv = 5)
return(specClust(x, nn = find_k$best.parameters$k, method = "symmetric")$cluster)}
f5 <- function(x){find_k <- train.kknn(x[,ncol(x)] ~ .,
data = x,
kmax = 35,
kernel = "gaussian",
kcv = 5)
return(specClust(x, nn = find_k$best.parameters$k, method = "random-walk")$cluster)}
#----------------------------------------------------------------------------------------
algos <- c(f1, f2, f3, f4, f5)
standard_deviations <- function(data, algorithm){
#data is our sample data
#algorithm is either 1, 2, 3, 4, or 5, where:
#1: kmeans 2:kmedoids 3:kmpp 4:spcc-symm 5:spcc-randomwalk
SSW <- integer(10)
SSB <- integer(10)
Time <- integer(10)
data$cluster = algos[[algorithm]](data)
cluster_performance(data, "cluster")
measures <- data.frame(SSW = SSW, SSB = SSB)
for(i in 1:10) {
t = proc.time()
data$cluster = algos[[algorithm]](data)
measures[i,] = cluster_performance(data, "cluster")
Time[i] = (proc.time() - t)[2]
}
return(cbind(measures, Time))
}
#data = f1(sample_data)
#Running Everything
#----------------------------------------------------------------------------------------
p1 = proc.time()
km_repeated = standard_deviations(sample_data, 1)
p1 = (proc.time() - p1)
p2 = proc.time()
kmed_repeated = standard_deviations(sample_data, 2)
p2 = (proc.time() - p2)
p3 = proc.time()
kmpp_repeated = standard_deviations(sample_data, 3)
p3 = (proc.time() - p3)
p4 = proc.time()
spc_sim = standard_deviations(sample_data, 4)
p4 = (proc.time() - p4)
p5 = proc.time()
spc_rwalk = standard_deviations(sample_data, 5)
p5 = (proc.time() - p5)
output <- data.frame("Mean SSW" = c(mean(km_repeated[,1]),
mean(kmed_repeated[,1]),
mean(kmpp_repeated[,1]),
mean(spc_sim[,1]),
mean(spc_rwalk[,1])),
"Mean SSB" = c(mean(km_repeated[,2]),
mean(kmed_repeated[,2]),
mean(kmpp_repeated[,2]),
mean(spc_sim[,2]),
mean(spc_rwalk[,2])),
"SD SSW" = c(sd(km_repeated[,1]),
sd(kmed_repeated[,1]),
sd(kmpp_repeated[,1]),
sd(spc_sim[,1]),
sd(spc_rwalk[,1])),
"SD SSB" = c(sd(km_repeated[,2]),
sd(kmed_repeated[,2]),
sd(kmpp_repeated[,2]),
sd(spc_sim[,2]),
sd(spc_rwalk[,2])),
"Mean Run Times" = c(mean(km_repeated[,3]),
mean(kmed_repeated[,3]),
mean(kmpp_repeated[,3]),
mean(spc_sim[,3]),
mean(spc_rwalk[,3])),
"SD Run Times" = c(sd(km_repeated[,3]),
sd(kmed_repeated[,3]),
sd(kmpp_repeated[,3]),
sd(spc_sim[,3]),
sd(spc_rwalk[,3])),
"Overall Time" = c(p1[3], p2[3], p3[3], p4[3], p5[3]))
write.csv(output, file = "clustering_output.csv")
output
print(head(spc_rwalk))
calculate_z_scores <- function(data,
max_number_of_centers,
outside_parameter,
size_parameter_space,
means_of_means=TRUE,
no_permutations=1000
)
{
print(paste("Beginning z-score calculation at ", Sys.time(),sep=""))
z<-matrix(-99, max_number_of_centers, max_number_of_centers-1)
for(i in 1:ncol(z)){ # cluster configuration (i.e. data set partitioned into 2, 3, 4 clusters)
for (j in 1:(i+1)){ # number of the cluster (first, second cluster etc.)
clust<-data[which(data[,i+size_parameter_space]==j),outside_parameter] #the cluster affiliations should be appended to the parameter space
if(length(clust)!=0){ #it could happen, that by removing the excluded sample, entire clusters are removed, not just single data points
cluster_col <- length(clust)
if(means_of_means){
resample_array<-sapply(1:no_permutations,
function(x) mean(as.numeric(sample(data[,outside_parameter],size=length(clust),replace=FALSE)))
)
}else{
resample_array<-sapply(1:no_permutations,
function(x) (sample(data[,outside_parameter],size=length(clust),replace=FALSE))
)
}
b_star <- mean(as.numeric(data[which(data[,i+size_parameter_space]==j),outside_parameter]), na.rm=TRUE)
b_ <- mean(as.numeric(resample_array), na.rm=TRUE)
sigma_b_star <- sd(as.numeric(resample_array), na.rm=TRUE) # sample s.d.
z[j,i] <- (b_star-b_)/sigma_b_star #z-score formula; No absolute values
if (is.infinite(z[j,i])) {
z[j,i]<-0 # if z-score infinite, set to 0
}
if (is.na(z[j,i])) {
z[j,i]<-0 # if z-score na, set to 0
}
}else{
z[j,i]<-0
}
}
}
print(paste("Z-score calculation completed at ", Sys.time(),sep=""))
return(z)
}
14*3
14*4
15*3
15*4
#==================================================================================================
# Title:    Text NLP
# Author:   Nura Kawa
# Summary:
#
#==================================================================================================
# Libraries
#--------------------------------------------------------------------------------------------------
library(plyr)
library(stringr)
library(tm)
# Setup
#--------------------------------------------------------------------------------------------------
envpath <- "C:/Users/Nura/Desktop/Independent Research/"
setwd(envpath)
# Download datasets from corpora
#--------------------------------------------------------------------------------------------------
BBC_articles_classes <- read.csv("C:/Users/Nura/Desktop/Independent Research/Data/BBC_articles_classes.csv", stringsAsFactors=FALSE)
View(BBC_articles_classes)
text <- BBC_articles_classes$Text
class(text)
length(text)
text <- paste(text, collapse = " ")
length(text)
text
rm(text)
text <- tm_map(text, removePunctuation)
class(text)
text <- BBC_articles_classes$Text
class(text)
text <- tm_map(text, removePunctuation)
text <- BBC_articles_classes$Text
class(text)
head(text)
text[1]
practice <- text[1] #first article
practice <- tm_map(practice, removePunctuation)
practice
?tm_map
Needed <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust", "cluster", "igraph", "fpc")
install.packages(Needed, dependencies=TRUE)
install.packages(Needed, dependencies = TRUE)
install.packages("Rcampdf", repos = "http://datacube.wu.ac.at/", type = "source")
envpath <- "C:/Users/Nura/Desktop/Independent Research/BBC/bbc"
setwd(envpath)
dir(envpath)
dir(paste0(envpath, "/business")
)
business_envpath <- dir(paste0(envpath, "/business"))
cname <- file.path(business_envpath)
dir(cname)
cname
library(tm)
business_docs <- Corpus(DirSource(cname))
cname <- file.path(business_envpath, "texts")
cname
business_envpath <- file.path(envpath, "business"))
business_envpath <- file.path(envpath, "business")
business_envpath
dir(business_envpath)
business_docs <- Corpus(DirSource(business_envpath))
summary(business_docs)
inspect(business_docts[1])
inspect(business_docs[1])
inspect(business_docs[2])
business_docs[[2]]
summary(business_docs)
summary(business_docs[[1]])
dir(business_envpath)
inspect(business_docs[1])
business_docs <- tm_map(business_docs, removePunctuation)
inspect(business_docs[1])
inspect(business_docs[[1]])
inspect(business_docs)
inspect(business_docs[1])
business_envpath
# Libraries
#--------------------------------------------------------------------------------------------------
library(plyr)
library(stringr)
needed <- c("tm","SnowballCC","RColorBrewer","ggplot2",
"wordcloud","biclust","cluster","igraph","fpc")
#install.packages(Needed, dependencies=TRUE)
#install.packages("Rcampdf", repos = "http://datacube.wu.ac.at/", type = "source")
envpath <- "C:/")
envpath <- "C:/"
rm(envpath)
cname <- file.path("C:", "cp-business")
cname
dir(cname)
business_docs <- Corpus(DirSource(cname))
summary(business_docs)
class(business_docs)
inspect(business_docs[2])
business_docs <- tm_map(business_docs, removePunctuation)
as.character(business_docs[[1]])
?removeWords
business_docs <- removeWords(business_docs, "")
cname <- file.path("C:", "cp-business")
business_docs <- Corpus(DirSource(cname))
source("import_bbc_data.R")
getwd()
source("C:/Users/Nura/Desktop/Independent Research/Scripts/import_bbc_data.R")
source("C:/Users/Nura/Desktop/Independent Research/Scripts/clean_bbc_data.R")
all_text <- BBC_df$Text
length(all_text)
all_text_corpus <- Corpus(all_text)
BBC_corpus <- Corpus(VectorSource(BBC_df$Text))
rm(all_text)
summary(BBC_corpus)
inspect(BBC_corpus[[1]])
inspect(BBC_corpus[1])
inspect(as.character(BBC_corpus[1]))
as.character(BBC_corpus[1])
as.character(BBC_corpus[2])
as.character(BBC_corpus[3])
BBC_corpus <- tm_map(BBC_corpus, removePunctuation)
as.character(BBC_corpus)
for(j in seq(BBC_corpus))
{
BBC_corpus[[j]] <- gsub("/", " ", BBC_corpus[[j]])
BBC_corpus[[j]] <- gsub("@", " ", BBC_corpus[[j]])
BBC_corpus[[j]] <- gsub("\\|", " ", BBC_corpus[[j]])
}
as.character(BBC_corpus[1])
as.character(BBC_corpus[2])
as.character(BBC_corpus[3])
lapply(BBC_corpus, length)
lapply(as.character(BBC_corpus), length)
?mapply
BBC_corpus[[1]]
BBC_corpus[[2]]
BBC_corpus[[224]]
BBC_corpus <- tm_map(BBC_corpus, removeNumbers)
?tm_map
business_docs <- tm_map(business_docs, removeNumbers)
source("C:/Users/Nura/Desktop/Independent Research/Scripts/clean_bbc_data.R")
BBC_corpus <- Corpus(VectorSource(BBC_df$Text))
BBC_corpus <- tm_map(BBC_corpus, removePunctuation)
BBC_corpus <- tm_map(BBC_corpus, removeNumbers)
BBC_corpus[1]
BBC_corpus[[1]]
as.character(BBC_corpus[[1]])
our_sample <- BBC_corpus[[1]]
Encoding(our_sample) <- "latin1"
class(our_sample)
class(as.character(BBC_corpus[[1]]))
our_sample <- as.character(BBC_corpus[[1]])
our_sample
Encoding(our_sample) <- "latin1"
our_sample <- strsplit(our_sample, " ")[[1]]
our_sample
Encoding(our_sample) <- "latin1"
iconv(our_sample, "latin1", "ASCII", sub="")
our_sample <- iconv(our_sample, "latin1", "ASCII", sub="")
BBC_corpus <- tm_map(BBC_corpus, tolower)
BBC_corpus <- tm_map(BBC_corpus, removeWords, stopwords("english"))
our_sample <- as.character(BBC_corpus[[1]])
our_sample
library(SnowballC)
BBC_corpus <- tm_map(BBC_corpus, stemDocument)
BBC_corpus <- tm_map(BBC_corpus, stripWhitespace)
our_sample <- as.character(BBC_corpus[[1]])
our_sample
BBC_corpus <- tm_map(BBC_corpus, removeWords, c("m"))
our_sample <- as.character(BBC_corpus[[1]])
our_sample
BBC_corpus <- tm_map(BBC_corpus, stripWhitespace)
our_sample <- as.character(BBC_corpus[[1]])
our_sample
BBC_corpus <- tm_map(BBC_corpus, stemDocument)
our_sample <- as.character(BBC_corpus[[1]])
our_sample
BBC_corpus <- tm_map(BBC_corpus, stemDocument)
BBC_corpus <- tm_map(BBC_corpus, replaceSynonyms, synonyms)
our_sample <- as.character(BBC_corpus[[1]])
our_sample
wordStem()
wordStem("runnin")
wordStem("running")
wordStem("baked")
wordStem("playing")
wordStem("reading")
wordStem("played")
BBC_corpus
our_sample <- as.character(BBC_corpus[[1]])
as.character(BBC_corpus[[1]])
?stemDocument
BBC_corpus <- stemDocument(BBC_corpus)
wordStem("Nura")
wordStem("playing")
wordStem("ruining")
wordStem("breathed")
BBC_corpus <- tm_map(BBC_corpus, PlainTextDocument)
dtm <- DocumentTermMatrix(BBC_corpus)
dtm
inspect(dtm)
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq)
ord
dtm[1:10, 1:10]
as.matrix(dtm)[1:10,1:10]
as.matrix(dtm)[100:110,1:10]
as.matrix(dtm)[100:110,111:111]
sum(dtm)
